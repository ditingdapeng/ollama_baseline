# ç”„å¬›å¿«é€Ÿæ¨¡å‹éƒ¨ç½²æŒ‡å—

åŸºäº Qwen2.5-0.5B + LoRA å¾®è°ƒçš„ç”„å¬›è§’è‰²æ¨¡å‹éƒ¨ç½²æŒ‡å—

## ğŸ“‹ å‰ææ¡ä»¶

### 1. ç¡®è®¤è®­ç»ƒå®Œæˆ
ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨ï¼š
```
training/training/models/huanhuan_fast/
â”œâ”€â”€ adapter_config.json
â”œâ”€â”€ adapter_model.safetensors
â”œâ”€â”€ train_results.json
â””â”€â”€ README.md
```

### 2. å®‰è£… Ollama
```bash
# macOS/Linux
curl -fsSL https://ollama.ai/install.sh | sh

# éªŒè¯å®‰è£…
ollama --version
```

## ğŸš€ å¿«é€Ÿéƒ¨ç½²

### æ–¹æ³•ä¸€ï¼šè‡ªåŠ¨éƒ¨ç½²ï¼ˆæ¨èï¼‰
```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd /Users/dapeng/Code/study/ollama

# è¿è¡Œéƒ¨ç½²è„šæœ¬
python deployment/deploy_huanhuan_fast.py
```

### æ–¹æ³•äºŒï¼šæ‰‹åŠ¨éƒ¨ç½²
```bash
# 1. å¯åŠ¨ Ollama æœåŠ¡
ollama serve

# 2. æ‹‰å–åŸºç¡€æ¨¡å‹
ollama pull qwen2.5:0.5b

# 3. åˆ›å»ºæ¨¡å‹ï¼ˆéœ€è¦å…ˆåˆ›å»º Modelfileï¼‰
ollama create huanhuan-qwen-fast -f deployment/Modelfile.huanhuan_fast

# 4. æµ‹è¯•æ¨¡å‹
ollama run huanhuan-qwen-fast
```

## ğŸ”§ éƒ¨ç½²è„šæœ¬åŠŸèƒ½

### åŸºæœ¬ç”¨æ³•
```bash
# å®Œæ•´éƒ¨ç½²
python deployment/deploy_huanhuan_fast.py

# æŒ‡å®šæ¨¡å‹è·¯å¾„
python deployment/deploy_huanhuan_fast.py --model-path training/training/models/huanhuan_fast

# ä»…æµ‹è¯•å·²éƒ¨ç½²çš„æ¨¡å‹
python deployment/deploy_huanhuan_fast.py --test-only

# æ˜¾ç¤ºéƒ¨ç½²ä¿¡æ¯
python deployment/deploy_huanhuan_fast.py --info-only

# åˆ é™¤æ¨¡å‹
python deployment/deploy_huanhuan_fast.py --remove-model huanhuan-qwen-fast
```

### éƒ¨ç½²æµç¨‹
1. âœ… æ£€æŸ¥ Ollama å®‰è£…
2. âœ… å¯åŠ¨ Ollama æœåŠ¡
3. âœ… æ‹‰å– Qwen2.5-0.5B åŸºç¡€æ¨¡å‹
4. âœ… åˆ›å»ºåŒ…å« LoRA é€‚é…å™¨çš„ Modelfile
5. âœ… åˆ›å»º Ollama æ¨¡å‹
6. âœ… æµ‹è¯•æ¨¡å‹å¯¹è¯æ•ˆæœ

## ğŸ’¬ ä½¿ç”¨æ¨¡å‹

### å‘½ä»¤è¡Œå¯¹è¯
```bash
ollama run huanhuan-qwen-fast
```

### API è°ƒç”¨
```bash
# å•æ¬¡å¯¹è¯
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "huanhuan-qwen-fast",
    "prompt": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±",
    "stream": false
  }'

# æµå¼å¯¹è¯
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "huanhuan-qwen-fast",
    "prompt": "èƒ½ä¸ºæˆ‘ä½œä¸€é¦–è¯—å—ï¼Ÿ",
    "stream": true
  }'
```

### Python API è°ƒç”¨
```python
import requests

def chat_with_huanhuan(prompt):
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": "huanhuan-qwen-fast",
            "prompt": prompt,
            "stream": False
        }
    )
    
    if response.status_code == 200:
        return response.json()["response"]
    else:
        return f"é”™è¯¯: {response.status_code}"

# ä½¿ç”¨ç¤ºä¾‹
print(chat_with_huanhuan("ä½ å¥½ï¼Œç”„å¬›"))
print(chat_with_huanhuan("ä½ è§‰å¾—å®«å»·ç”Ÿæ´»å¦‚ä½•ï¼Ÿ"))
```

## ğŸ“Š æ¨¡å‹ç‰¹ç‚¹

### æŠ€æœ¯è§„æ ¼
- **åŸºç¡€æ¨¡å‹**: Qwen2.5-0.5B (çº¦ 500M å‚æ•°)
- **å¾®è°ƒæ–¹æ³•**: LoRA (Low-Rank Adaptation)
- **LoRA é…ç½®**:
  - rank (r): 2
  - alpha: 4
  - dropout: 0.1
  - target_modules: q_proj
- **è®­ç»ƒæ—¶é—´**: ~70 ç§’
- **å¯è®­ç»ƒå‚æ•°**: 86,016 (0.02%)

### è§’è‰²ç‰¹ç‚¹
- ğŸ­ **è§’è‰²**: ç”„å¬›ï¼ˆã€Šç”„å¬›ä¼ ã€‹å¥³ä¸»è§’ï¼‰
- ğŸ’¬ **è¯­è¨€é£æ ¼**: å¤å…¸é›…è‡´ï¼Œä½¿ç”¨"è‡£å¦¾"è‡ªç§°
- ğŸ›ï¸ **èƒŒæ™¯**: å¤§ç†å¯ºå°‘å¿ç”„è¿œé“ä¹‹å¥³ï¼Œåæˆä¸ºç†¹è´µå¦ƒ
- ğŸ“š **ç‰¹é•¿**: è¯—è¯æ­Œèµ‹ï¼Œæ¸©å©‰è´¤æ·‘

### æ€§èƒ½ä¼˜åŠ¿
- âš¡ **å¿«é€Ÿæ¨ç†**: åŸºäº 0.5B å°æ¨¡å‹ï¼Œæ¨ç†é€Ÿåº¦å¿«
- ğŸ’¾ **å†…å­˜å‹å¥½**: æ˜¾å­˜éœ€æ±‚ä½ï¼Œé€‚åˆä¸ªäººè®¾å¤‡
- ğŸ¯ **è§’è‰²ä¸€è‡´**: LoRA å¾®è°ƒä¿æŒè§’è‰²ç‰¹æ€§
- ğŸ”§ **æ˜“äºéƒ¨ç½²**: æ”¯æŒ Ollama ç”Ÿæ€ç³»ç»Ÿ

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. Ollama æœåŠ¡æ— æ³•å¯åŠ¨
```bash
# æ£€æŸ¥ç«¯å£å ç”¨
lsof -i :11434

# æ‰‹åŠ¨å¯åŠ¨æœåŠ¡
ollama serve
```

#### 2. åŸºç¡€æ¨¡å‹ä¸‹è½½å¤±è´¥
```bash
# æ‰‹åŠ¨æ‹‰å–æ¨¡å‹
ollama pull qwen2.5:0.5b

# æ£€æŸ¥ç½‘ç»œè¿æ¥
curl -I https://ollama.ai
```

#### 3. æ¨¡å‹åˆ›å»ºå¤±è´¥
```bash
# æ£€æŸ¥ Modelfile è¯­æ³•
cat deployment/Modelfile.huanhuan_fast

# åˆ é™¤å·²å­˜åœ¨çš„æ¨¡å‹
ollama rm huanhuan-qwen-fast

# é‡æ–°åˆ›å»º
ollama create huanhuan-qwen-fast -f deployment/Modelfile.huanhuan_fast
```

#### 4. æ¨¡å‹å›ç­”è´¨é‡é—®é¢˜
- æ£€æŸ¥è®­ç»ƒæ•°æ®è´¨é‡
- è°ƒæ•´æ¨¡å‹å‚æ•°ï¼ˆtemperature, top_p ç­‰ï¼‰
- é‡æ–°è®­ç»ƒæ¨¡å‹

### æ—¥å¿—æŸ¥çœ‹
```bash
# æŸ¥çœ‹ Ollama æ—¥å¿—
ollama logs

# æŸ¥çœ‹éƒ¨ç½²è„šæœ¬æ—¥å¿—
python deployment/deploy_huanhuan_fast.py --info-only
```

## ğŸ“ æ–‡ä»¶ç»“æ„

```
deployment/
â”œâ”€â”€ deploy_huanhuan_fast.py      # å¿«é€Ÿéƒ¨ç½²è„šæœ¬
â”œâ”€â”€ huanhuan_deploy.py           # é€šç”¨éƒ¨ç½²è„šæœ¬
â”œâ”€â”€ Modelfile.huanhuan_fast      # Ollama Modelfile (è‡ªåŠ¨ç”Ÿæˆ)
â””â”€â”€ FAST_DEPLOYMENT_GUIDE.md     # æœ¬æŒ‡å—

training/training/models/huanhuan_fast/
â”œâ”€â”€ adapter_config.json          # LoRA é…ç½®
â”œâ”€â”€ adapter_model.safetensors    # LoRA æƒé‡
â”œâ”€â”€ train_results.json           # è®­ç»ƒç»“æœ
â””â”€â”€ README.md                    # æ¨¡å‹è¯´æ˜
```

## ğŸ”„ æ¨¡å‹ç®¡ç†

### åˆ—å‡ºæ‰€æœ‰æ¨¡å‹
```bash
ollama list
```

### åˆ é™¤æ¨¡å‹
```bash
ollama rm huanhuan-qwen-fast
```

### æ›´æ–°æ¨¡å‹
```bash
# åˆ é™¤æ—§æ¨¡å‹
ollama rm huanhuan-qwen-fast

# é‡æ–°éƒ¨ç½²
python deployment/deploy_huanhuan_fast.py
```

## ğŸ“ˆ ä¸‹ä¸€æ­¥

1. **Web ç•Œé¢**: å¼€å‘åŸºäº Streamlit çš„å¯¹è¯ç•Œé¢
2. **API æœåŠ¡**: åˆ›å»º FastAPI æœåŠ¡åŒ…è£… Ollama
3. **æ¨¡å‹ä¼˜åŒ–**: è°ƒæ•´ LoRA å‚æ•°æå‡æ•ˆæœ
4. **æ•°æ®æ‰©å……**: å¢åŠ è®­ç»ƒæ•°æ®æå‡å¯¹è¯è´¨é‡
5. **å¤šè½®å¯¹è¯**: æ”¯æŒä¸Šä¸‹æ–‡è®°å¿†çš„å¯¹è¯

## ğŸ“ æ”¯æŒ

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. Ollama æ˜¯å¦æ­£ç¡®å®‰è£…å’Œè¿è¡Œ
2. æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´
3. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸
4. ç³»ç»Ÿèµ„æºæ˜¯å¦å……è¶³

---

**éƒ¨ç½²å®Œæˆåï¼Œä½ å°±å¯ä»¥ä¸ç”„å¬›è¿›è¡Œå¯¹è¯äº†ï¼** ğŸ‰